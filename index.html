<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Deep-vessel by KGPML</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Deep-vessel</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/KGPML/Deep-Vessel" class="btn">View on GitHub</a>
      <a href="https://github.com/KGPML/Deep-Vessel/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/KGPML/Deep-Vessel/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="ensemble-of-deep-convolutional-neural-networks-for-learning-to-detect-retinal-vessels-in-fundus-images" class="anchor" href="#ensemble-of-deep-convolutional-neural-networks-for-learning-to-detect-retinal-vessels-in-fundus-images" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Ensemble of Deep Convolutional Neural Networks for Learning to Detect Retinal Vessels in Fundus Images</h1>

<p>Vision impairment due to pathological damage of the retina can largely be prevented through periodic screening using fundus color imaging. However the challenge with large scale screening is the inability to exhaustively detect fine blood vessels crucial to disease diagnosis. This work presents a computational imaging framework using deep and ensemble learning for reliable detection of blood vessels in fundus color images. An ensemble of deep convolutional neural networks is trained to segment vessel and non-vessel areas of a color fundus image. During inference, the responses of the individual ConvNets of the ensemble are averaged to form the final segmentation. In experimental evaluation with the DRIVE database, we achieve the objective of vessel detection with maximum average accuracy of 91.8% (This accuracy is different from the accuracy reported in the <a href="http://arxiv.org/abs/1603.04833">paper</a> because of different libraries used) and Kappa Score of 0.7031.</p>

<hr>

<table>
<thead>
<tr>
<th align="center">FUNDUS Image</th>
<th align="center">Manual Segmentation</th>
<th align="center">Predicted Segmentation</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/01_test_src.jpg?raw=True" width="220"></td>
<td align="center"><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/01_manual1.jpg?raw=True" width="220"></td>
<td align="center"><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/01_test.jpg?raw=True" width="220"></td>
</tr>
</tbody>
</table>

<hr>

<h2>
<a id="proposed-method" class="anchor" href="#proposed-method" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Proposed Method</h2>

<p><strong>Ensemble learning</strong> is a technique of using multiple models or experts for solving a particular artificial intelligence problem. Ensemble methods seek to promote diversity among the models they combine and reduce the problem related to overfitting of the training data. The outputs of the individual models of the ensemble are combined (e.g. by averaging) to form the final prediction.</p>

<p><strong>Convolutional neural networks</strong> (CNN or ConvNet) are a special category of artificial neural networks designed for processing data with a gridlike structure. The ConvNet architecture is based on sparse interactions and parameter sharing and is highly effective for efficient learning of spatial invariances in images. There are four kinds of layers in a typical ConvNet architecture: convolutional (conv), pooling (pool), fullyconnected (affine) and rectifying linear unit (ReLU). Each convolutional layer transforms one set of feature maps into another set of feature maps by convolution with a set of filters.</p>

<p>This paper makes an attempt to ameliorate the issue of subjectivity induced bias in feature representation by training an ensemble of 12 Convolutional Neural Networks (ConvNets) on raw color fundus images to discriminate vessel pixels from non-vessel ones. </p>

<p><strong>Dataset</strong>: The ensemble of ConvNets is evaluated by learning with the DRIVE training set (image id. 21-40) and
testing over the DRIVE test set (image id. 1-20). </p>

<p><strong>Learning mechanism</strong>: Each ConvNet is trained independently on a set of 120000 randomly chosen 3×31×31 patches.
Learning rate was kept constant across models at 5e − 4. Dropout probability and number of hidden units in
the penultimate affine layer of the different models were sampled respectively from U ([0.5, 0.9]) and U ({128, 256, 512}) where U(.) denotes uniform probability distribution over a given range. The models were trained using Adam algorithm with minibatch size 256. Some of these parameters are different from the paper. The user can set some of these parameters using command line arguments which is explained in later sections.</p>

<p><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/Proposed-Method.jpg?raw=True" width="800"></p>

<hr>

<h2>
<a id="architecture" class="anchor" href="#architecture" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Architecture</h2>

<p>The ConvNets have the same organization of layers which can be described as: </p>

<p><strong>input- [conv - relu]-[conv - relu - pool] x 2 - affine - relu - [affine with dropout] - softmax</strong></p>

<p>(Schematic representation below)</p>

<p><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/Architecture.jpg?raw=True" width="800"></p>

<p>The system was trained on a machine with dual Intel Xeon E5-2630 v2 CPUs, 32 GB RAM and NVIDIA Tesla K-20C GPU. Average training time for each model was 3.5 hours (for 10000 epochs). Average inference time for each image was 55 secs on the said machine.</p>

<hr>

<h2>
<a id="some-results" class="anchor" href="#some-results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Some Results</h2>

<table>
<thead>
<tr>
<th align="center">FUNDUS Image</th>
<th align="center">Magnified Section</th>
<th align="center">Ground Truth</th>
<th align="center">Prediction</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/Magnified1_1.jpg?raw=True" width="180"></td>
<td align="center"><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/Magnified1_2.jpg?raw=True" width="180"></td>
<td align="center"><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/Magnified1_3.jpg?raw=True" width="180"></td>
<td align="center"><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/Magnified1_4.jpg?raw=True" width="180"></td>
</tr>
<tr>
<td align="center"><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/Magnified2_1.jpg?raw=True" width="180"></td>
<td align="center"><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/Magnified2_2.jpg?raw=True" width="180"></td>
<td align="center"><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/Magnified2_3.jpg?raw=True" width="180"></td>
<td align="center"><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/Magnified2_4.jpg?raw=True" width="180"></td>
</tr>
<tr>
<td align="center"><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/Magnified3_1.jpg?raw=True" width="180"></td>
<td align="center"><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/Magnified3_2.jpg?raw=True" width="180"></td>
<td align="center"><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/Magnified3_3.jpg?raw=True" width="180"></td>
<td align="center"><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/Magnified3_4.jpg?raw=True" width="180"></td>
</tr>
<tr>
<td align="center"><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/Magnified4_1.jpg?raw=True" width="180"></td>
<td align="center"><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/Magnified4_2.jpg?raw=True" width="180"></td>
<td align="center"><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/Magnified4_3.jpg?raw=True" width="180"></td>
<td align="center"><img src="https://github.com/KGPML/Deep-Vessel/blob/master/images/Magnified4_4.jpg?raw=True" width="180"></td>
</tr>
</tbody>
</table>

<p><strong>Note</strong> that in the 3rd image the blood vessels are not easily visible to the human eye but our network does a good job at discerning the fine structure of the vessel.</p>

<p>The Ensemble of ConvNets efficiently captures the underlying statistics that govern the degree of vesselness of a point in a color fundus image. This is particularly demonstrated in the 4th row, where the Ensemble detects a clinically important condition called <a href="https://en.wikipedia.org/wiki/Neovascularization">Neovascularization</a> (which we got verified by multiple ophthalmologists) not marked in the ground truth.</p>

<hr>

<hr>

<h2>
<a id="setup" class="anchor" href="#setup" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Setup</h2>

<p>Download the DRIVE dataset from <a href="http://www.isi.uu.nl/Research/Databases/DRIVE/">this link</a>. In order to run this code smoothly without having to change the code, please set up the directory tree in a way similar to the tree structure presented below.</p>

<pre><code>Project
|-- Data
|   |-- DRIVE
|   |   |-- test
|   |   |   |-- Contains 4 folders
|   |   |-- training
|   |   |   |-- Contains 4 folders
|   |-- models
|   |   |-- This folder is auto-generated by the code 
|   |   |-- It contains the saved models
|   |-- logs
|   |   |-- Event files necessary for Tensorboard (Auto-generated folder)
|-- Deep-Vessel
|   |-- Notebooks
|   |   |-- Contains necessary notebooks for development
|   |-- Scripts
|   |   |-- Contains scripts needed to preprocess, train and deploy the models
|   |-- arglists
|   |   |-- JSON files with required parameters for each model
|   |-- images
|   |   |-- Contains images for website. You may delete this folder
|   |-- README.md
</code></pre>

<hr>

<h2>
<a id="usage" class="anchor" href="#usage" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2>

<h3>
<a id="scripts" class="anchor" href="#scripts" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Scripts</h3>

<p>All these python scripts can be invoked with <code>--help</code> to display a brief help message.</p>

<ul>
<li>
<code>Preprocessor.py</code> crops random pathces from all training images and saves them in a PANDAS DataFrame</li>
<li>
<code>v2_graph.py</code> trains a single convolutional network </li>
<li>
<code>train_ensemble.py</code> trains an esemble of convolutional networks with different parameters</li>
<li>
<code>Test.py</code> decodes the test images</li>
<li>
<code>ensemble_decoder.py</code> decodes the test images using an ensemble of different saved models</li>
<li>
<code>create_json.py</code> small utility script to create a json file with model args which can be edited later in a text file</li>
<li>
<code>SuperPixel_Decoder.py</code> is an experimental fast SuperPixel based decoder, but isn't very accurate and hence is not updated</li>
</ul>

<p>Make sure to run these scripts from within the <code>Scripts</code> folder, otherwise it may throw an <code>IOError</code> as the paths used are relative</p>

<h5>
<a id="preprocessorpy" class="anchor" href="#preprocessorpy" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Preprocessor.py</h5>

<pre><code>Usage: Preprocessor.py [OPTIONS]

Options:
  --total_patches TOTAL_PATCHES 
                        Total number of training images/patches to be used [Default - 4800]
  --patch_dim PATCH_DIM
                        Dimension of window to be used as a training patch [Default - 31]
  --positive POSITIVE   Proportion of positive classes to be kept in training data [Default - 0.5]
</code></pre>

<p>Example usage:</p>

<p>We used</p>

<pre><code>python Preprocessor.py --total_patches 120000 
</code></pre>

<p>To save <code>30000</code> patches with dimension of <code>3*31*31</code> and a <code>4:1</code> proportion of positive classes, use</p>

<pre><code>python Preprocessor.py --total_patches 30000 --patch_dim 15 --positive 0.8
</code></pre>

<h5>
<a id="v2_graphpy" class="anchor" href="#v2_graphpy" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>v2_graph.py</h5>

<pre><code>Usage: v2_graph.py [OPTIONS]

Options:
  --batch BATCH                           Batch Size [Default - 64]
  --fchu1 FCHU1                           Number of hidden units in FC1 layer [Default - 512]
  --learning_rate LEARNING_RATE           Learning rate for optimiser [Default - 5e-4]
  --training_prop TRAINING_PROP           Proportion of data to be used for training data [Default - 0.8]
  --max_steps MAX_STEPS                   Maximum number of iteration till which the program must run [Default - 100]
  --checkpoint_step CHECKPOINT_STEP       Step after which an evaluation is carried out on validation set and model is saved [Default - 50]
  --loss_step LOSS_STEP                   Step after which loss is printed [Default - 5]
  --keep_prob KEEP_PROB                   Keep Probability for dropout layer [Default - 0.5]
  --model_name MODEL_NAME                 Index of the model [Default - '1']
</code></pre>

<p>Example usage:</p>

<p>We used</p>

<pre><code>python v2_graph.py --batch 256 --learning_rate 5e-4 --training_prop 0.9 --max_steps 8000 --checkpoint_step 400 --loss_step 25 
</code></pre>

<h5>
<a id="train_ensemblepy" class="anchor" href="#train_ensemblepy" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>train_ensemble.py</h5>

<pre><code>Usage: train_ensemble.py [OPTIONS]

Options:
   --a A       Path to JSON file containing model arguments
</code></pre>

<p>Example usage:</p>

<p>We used</p>

<pre><code>python train_ensemble.py --a ../arglists/heavy.json
</code></pre>

<h5>
<a id="testpy" class="anchor" href="#testpy" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Test.py</h5>

<pre><code>Usage: Test.py [OPTIONS]

Options:
  --fchu1 FCHU1    Number of hidden units in FC1 layer. This should be identical to the one used in the model 
                   [Default - 256]
  --out OUT        Directory to put rendered images to
  --inp INP        Directory containing images for testing
  --model MODEL    Path to the saved tensorflow model checkpoint
  --format FORMAT  Format to save the images in. [Available formats: npz, jpg and png]

</code></pre>

<p>Example usage:</p>

<p>We used</p>

<pre><code>python Test.py --fchu1 512 --format png --out ../../Data/DRIVE/tmp/ --inp ../../Data/DRIVE/test/ --model ../../Data/models/model1/model.ckpt-7999

</code></pre>

<h5>
<a id="ensemble_decoderpy" class="anchor" href="#ensemble_decoderpy" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>ensemble_decoder.py</h5>

<pre><code>Usage: ensemble_decoder.py [OPTIONS]

Options:
 --a A       Path to JSON file containing model arguments
 --m M       Path to JSON file containing saved model paths
 --out OUT   Directory to put rendered images to
 --inp INP   Directory containing images for testing

</code></pre>

<p>Example usage:</p>

<p>We used</p>

<pre><code>python ensemble_decoder.py --m ../arglists/model_paths_heavy.json --a ../arglists/heavy.json --out ../../Data/DRIVE/ensemble_test_results --inp ../../Data/DRIVE/test/

</code></pre>

<hr>

<h3>
<a id="configuration" class="anchor" href="#configuration" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Configuration</h3>

<p>The <code>arglists/</code> folder contains JSON files that store necessary command line arguments. It is human readable and hence can be edited easily by a user. </p>

<h5>
<a id="heavyjson" class="anchor" href="#heavyjson" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>heavy.json</h5>

<p>This is necessary for training the ensemble and needs to be passed to <code>train_ensemble.py</code>. It contains the list of arguments that needs to be passed to <code>v2_graph.py</code> for every model. As the name suggests, this configuration took us 2 days to run on a server, and unless you have a powerful GPU, I would suggest editing this file before running it.</p>

<h5>
<a id="model_paths_heavyjson" class="anchor" href="#model_paths_heavyjson" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>model_paths_heavy.json</h5>

<p>The models saved by training the ensemble are saved as checkpoints. This is a simple file that stores the paths to the best checkpoints for each model.</p>

<hr>

<h3>
<a id="acknowledgement" class="anchor" href="#acknowledgement" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Acknowledgement</h3>

<p>This repository is a TensorFlow re-implementation by <a href="https://in.linkedin.com/in/ankushchatterjee">Ankush Chatterjee</a> [during his internship with <a href="http://santara.github.io/">Anirban Santara</a> and <a href="http://cse.iitkgp.ac.in/%7Epabitra/">Pabitra Mitra</a> at the <a href="http://cse.iitkgp.ac.in/">Department of Computer Science and Engineering, IIT Kharagpur</a> during the summer of 2016] of the work done by <a href="https://www.linkedin.com/in/debapriya-maji-a66594102">Debapriya Maji</a>, <a href="http://santara.github.io/">Anirban Santara</a>, <a href="http://cse.iitkgp.ac.in/%7Epabitra/">Pabitra Mitra</a> and <a href="http://www.facweb.iitkgp.ernet.in/%7Edebdoot/">Debdoot Sheet</a>. Check out the original paper (<a href="http://arxiv.org/abs/1603.04833">http://arxiv.org/abs/1603.04833</a>) for more details. The authors would like to thank <a href="https://www.linkedin.com/in/drsujoykar">Dr. Sujoy Kar</a>, Dr. Tapas Paul and Dr. Asim Kumar Kandar from <a href="http://kolkata.apollohospitals.com/">Apollo Gleneagles Hospitals Kolkata</a> for valuable discussions in course of development of this system.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/KGPML/Deep-Vessel">Deep-vessel</a> is maintained by <a href="https://github.com/KGPML">KGPML</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
